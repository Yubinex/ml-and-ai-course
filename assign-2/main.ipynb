{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT NOTICE\n",
    "\n",
    "## This notebook requires a significant amount of time to run due to the extensive training of multiple models and optimization processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "raw_data = pd.read_csv('data/student_data.csv')\n",
    "raw_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming some columns\n",
    "raw_data.rename(columns={\n",
    "    'Nacionality': 'Nationality',\n",
    "    'Daytime/evening attendance\\t': 'Daytime/evening attendance'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()\n",
    "data = data[data['Target'] != 'Enrolled']\n",
    "data['Target'] = data['Target'].map({'Graduate': 1, 'Dropout': 0})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10, 5)})\n",
    "ax = sns.countplot(x=data['Target'])\n",
    "\n",
    "# Add legend\n",
    "legend_labels = {'Dropout': 0, 'Graduate': 1}\n",
    "palette = sns.color_palette()[:len(legend_labels)]\n",
    "handles = [plt.Rectangle((0, 0), 1, 1, color=palette[i]) for i in range(len(legend_labels))]\n",
    "ax.legend(handles, legend_labels.keys())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "ax = sns.countplot(x=data['Course'], hue=data['Target'])\n",
    "\n",
    "# Set legend labels\n",
    "legend_labels = {0: 'Dropout', 1: 'Graduate'}\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, legend_labels.values())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "ax = sns.countplot(x=data['Curricular units 1st sem (approved)'], hue=data['Target'])\n",
    "\n",
    "# Set legend labels\n",
    "legend_labels = {0: 'Dropout', 1: 'Graduate'}\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, legend_labels.values())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "ax = sns.countplot(x=data['Curricular units 2nd sem (approved)'], hue=data['Target'])\n",
    "\n",
    "# Set legend labels\n",
    "legend_labels = {0: 'Dropout', 1: 'Graduate'}\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, legend_labels.values())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "ax = sns.countplot(x=data['Daytime/evening attendance'], hue=data['Target'])\n",
    "\n",
    "# Set legend labels\n",
    "legend_labels = {0: 'Dropout', 1: 'Graduate'}\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, legend_labels.values())\n",
    "\n",
    "# Set x-axis tick labels\n",
    "tick_labels = {0: '0 - Evening', 1: '1 - Daytime'}\n",
    "plt.xticks(list(tick_labels.keys()), list(tick_labels.values()))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "ax = sns.countplot(x=data['Gender'], hue=data['Target'])\n",
    "\n",
    "# Set legend labels\n",
    "legend_labels = {0: 'Dropout', 1: 'Graduate'}\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, legend_labels.values())\n",
    "\n",
    "# Set x-axis tick labels\n",
    "tick_labels = {0: '0 - Female', 1: '1 - Male'}\n",
    "plt.xticks(list(tick_labels.keys()), list(tick_labels.values()))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "ax = sns.countplot(x=data['Scholarship holder'], hue=data['Target'])\n",
    "\n",
    "# Set legend labels\n",
    "legend_labels = {0: 'Dropout', 1: 'Graduate'}\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, legend_labels.values())\n",
    "\n",
    "# Set x-axis tick labels\n",
    "tick_labels = {0: 'No', 1: 'Yes'}\n",
    "plt.xticks(list(tick_labels.keys()), list(tick_labels.values()))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix between features and the target column\n",
    "correlation_matrix = data.corr()\n",
    "correlation_with_target = correlation_matrix['Target'].drop('Target')  # Exclude the target column itself\n",
    "\n",
    "# Plot the correlations with the target column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=correlation_with_target.index, y=correlation_with_target.values)\n",
    "plt.title('Correlation with Target Column')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Discuss the interesting correlation\n",
    "most_correlated_feature = correlation_with_target.abs().idxmax()\n",
    "correlation_value = correlation_with_target[most_correlated_feature]\n",
    "discussion = f\"The most interesting correlation is between '{most_correlated_feature}' and the target column with a correlation value of {correlation_value:.2f}.\\nThis indicates a strong relationship between these two variables.\"\n",
    "print(discussion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "The most interesting correlation is between 'Curricular units 2nd sem (approved)' and the target column with a correlation value of 0.66. This indicates a strong relationship between the number of curricular units approved in the second semester and the student's graduation or dropout status.\n",
    "\n",
    "A correlation value of 0.66 suggests a positive association between the number of approved curricular units in the second semester and the likelihood of a student being a graduate rather than a dropout. As the number of approved curricular units in the second semester increases, there is a higher probability of the student being classified as a graduate rather than a dropout.\n",
    "\n",
    "This correlation implies that students who successfully complete more curricular units in the second semester are more likely to graduate rather than drop out. It indicates that academic performance and progress in the second semester play a significant role in determining the student's outcome.\n",
    "\n",
    "Additionally, it is worth noting that a similar pattern can be observed for the correlation between the number of curricular units approved in the first semester and the student's graduation or dropout status. Although the correlation value for the first semester may differ, it also suggests a positive association between the number of approved curricular units in the first semester and the likelihood of being a graduate rather than a dropout.\n",
    "\n",
    "This consistency across both semesters reinforces the importance of early academic success and progress in predicting a student's outcome. It suggests that students who perform well and achieve a higher number of approved curricular units in both the first and second semesters are more likely to graduate. On the other hand, students who struggle or have fewer approved curricular units in either semester may be at a higher risk of dropping out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Target'], axis=1)\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rf = rf.predict(X_train)\n",
    "training_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "\n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "testing_acc_rf = accuracy_score(y_test, y_test_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize RandomForest Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with feature names and importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf.feature_importances_})\n",
    "\n",
    "# Sort the DataFrame by feature importances in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Create a bar plot for feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(f'Training Accuracy: {training_acc_rf}')\n",
    "print(f'Testing Accuracy: {testing_acc_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV optimization for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],       # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "\n",
    "# Fit the data to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "y_train_pred_rf_gs = best_rf.predict(X_train)\n",
    "y_test_pred_rf_gs = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "training_acc_rf_gs = accuracy_score(y_train, y_train_pred_rf_gs)\n",
    "testing_acc_rf_gs = accuracy_score(y_test, y_test_pred_rf_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_rf_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_rf_gs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize RandomForest with GridSearchCV Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with feature names and importances for the best estimator\n",
    "feature_importance_df_gs = pd.DataFrame({'Feature': X_train.columns, 'Importance': best_rf.feature_importances_})\n",
    "\n",
    "# Sort the DataFrame by feature importances in descending order\n",
    "feature_importance_df_gs = feature_importance_df_gs.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Create a bar plot for feature importances of the best estimator\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df_gs)\n",
    "plt.title('Feature Importances - Random Forest (GridSearchCV)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(f'Training Accuracy (GridSearchCV): {training_acc_rf_gs}')\n",
    "print(f'Testing Accuracy (GridSearchCV): {testing_acc_rf_gs}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_dt = dt.predict(X_train)\n",
    "training_acc_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "\n",
    "y_test_pred_dt = dt.predict(X_test)\n",
    "testing_acc_dt = accuracy_score(y_test, y_test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV optimization for DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10],       # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5)\n",
    "\n",
    "# Fit the data to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "y_train_pred_dt_gs = best_dt.predict(X_train)\n",
    "y_test_pred_dt_gs = best_dt.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "training_acc_dt_gs = accuracy_score(y_train, y_train_pred_dt_gs)\n",
    "testing_acc_dt_gs = accuracy_score(y_test, y_test_pred_dt_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_dt_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_dt_gs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_lr = lr.predict(X_train)\n",
    "training_acc_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "\n",
    "y_test_pred_lr = lr.predict(X_test)\n",
    "testing_acc_lr = accuracy_score(y_test, y_test_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV optimization for LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],            # Inverse of regularization strength\n",
    "    'penalty': ['l1', 'l2']       # Regularization penalty\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
    "\n",
    "# Fit the data to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_lr = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "y_train_pred_lr_gs = best_lr.predict(X_train)\n",
    "y_test_pred_lr_gs = best_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "training_acc_lr_gs = accuracy_score(y_train, y_train_pred_lr_gs)\n",
    "testing_acc_lr_gs = accuracy_score(y_test, y_test_pred_lr_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_lr_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_lr_gs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_svm = svm.predict(X_train)\n",
    "training_acc_svm = accuracy_score(y_train, y_train_pred_svm)\n",
    "\n",
    "y_test_pred_svm = svm.predict(X_test)\n",
    "testing_acc_svm = accuracy_score(y_test, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_svm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV optimization for Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],                  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],        # Kernel function\n",
    "    'gamma': ['scale', 'auto']          # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "\n",
    "# Fit the data to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training and testing sets\n",
    "y_train_pred_svm_gs = best_svm.predict(X_train)\n",
    "y_test_pred_svm_gs = best_svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "training_acc_svm_gs = accuracy_score(y_train, y_train_pred_svm_gs)\n",
    "testing_acc_svm_gs = accuracy_score(y_test, y_test_pred_svm_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "training_acc_svm_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "testing_acc_svm_gs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Scores in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the accuracy scores\n",
    "accuracy_scores = {\n",
    "    'Model': ['RandomForest', 'RandomForestGridSearch', 'DecisionTree', 'DecisionTreeGridSearch', \n",
    "              'LogisticRegression', 'LogisticRegressionGridSearch', 'SVM', 'SVMGridSearch'],\n",
    "    'Training Accuracy': [training_acc_rf, training_acc_rf_gs, training_acc_dt, training_acc_dt_gs,\n",
    "                           training_acc_lr, training_acc_lr_gs, training_acc_svm, training_acc_svm_gs],\n",
    "    'Testing Accuracy': [testing_acc_rf, testing_acc_rf_gs, testing_acc_dt, testing_acc_dt_gs,\n",
    "                          testing_acc_lr, testing_acc_lr_gs, testing_acc_svm, testing_acc_svm_gs]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the accuracy scores dictionary\n",
    "accuracy_df = pd.DataFrame(accuracy_scores)\n",
    "\n",
    "# Display the accuracy DataFrame\n",
    "print(accuracy_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy scores for each model\n",
    "models = ['RandomForest', 'RandomForestGridSearch', 'DecisionTree', 'DecisionTreeGridSearch',\n",
    "          'LogisticRegression', 'LogisticRegressionGridSearch', 'SVM', 'SVMGridSearch']\n",
    "training_acc = [training_acc_rf, training_acc_rf_gs, training_acc_dt, training_acc_dt_gs,\n",
    "                training_acc_lr, training_acc_lr_gs, training_acc_svm, training_acc_svm_gs]\n",
    "testing_acc = [testing_acc_rf, testing_acc_rf_gs, testing_acc_dt, testing_acc_dt_gs,\n",
    "               testing_acc_lr, testing_acc_lr_gs, testing_acc_svm, testing_acc_svm_gs]\n",
    "\n",
    "# Plotting the performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, training_acc, label='Training Accuracy')\n",
    "plt.bar(models, testing_acc, label='Testing Accuracy')\n",
    "plt.title('Model Performance - Training vs Testing Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional RandomForest (GridSearchCV) Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(best_rf, X, y, cv=5)\n",
    "\n",
    "# Calculate mean and standard deviation of accuracy scores\n",
    "mean_accuracy = cv_scores.mean()\n",
    "std_accuracy = cv_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cross-validated predictions\n",
    "cv_predictions = cross_val_predict(best_rf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y, cv_predictions)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Accuracy: {:.2f}\".format(mean_accuracy))\n",
    "print(\"Standard Deviation of Accuracy: {:.6f}\".format(std_accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create the \"out\" folder if it doesn't exist\n",
    "os.makedirs(\"out\", exist_ok=True)\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"out/predict_student_success.pkl\"\n",
    "\n",
    "# Save the model using pickle\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(best_rf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
